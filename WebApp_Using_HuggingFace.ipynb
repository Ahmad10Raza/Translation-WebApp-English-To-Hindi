{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English To Hindi Translation Using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How?\n",
    "\n",
    "Translating English to Hindi using Natural Language Processing (NLP) typically involves building a machine translation system. Here are the general steps you can follow:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Gather a large dataset of parallel English-Hindi sentences. These should be pairs of sentences where the English sentence corresponds to its Hindi translation.\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   - Tokenize the sentences into words or subwords.\n",
    "   - Convert the text into lowercase to ensure consistency.\n",
    "   - Remove any unnecessary characters, punctuation, or special symbols.\n",
    "\n",
    "3. **Tokenization:**\n",
    "   - Tokenize the sentences into individual words or subwords. This step is essential for representing the text in a format that the machine can understand.\n",
    "\n",
    "4. **Building a Neural Machine Translation Model:**\n",
    "   - Choose a suitable architecture for your neural network. Sequence-to-sequence models with attention mechanisms are commonly used for machine translation tasks.\n",
    "   - Implement and train your model using frameworks like TensorFlow or PyTorch.\n",
    "   - Split your dataset into training and validation sets.\n",
    "\n",
    "5. **Training:**\n",
    "   - Train your model on the training dataset. This involves feeding the English sentences as input and the corresponding Hindi sentences as output, adjusting the model's parameters to minimize the translation error.\n",
    "\n",
    "6. **Validation:**\n",
    "   - Use the validation set to monitor the performance of your model during training. Adjust hyperparameters if necessary to prevent overfitting.\n",
    "\n",
    "7. **Evaluation:**\n",
    "   - Once the model is trained, evaluate its performance on a separate test set that it has never seen before. Common evaluation metrics include BLEU score and METEOR.\n",
    "\n",
    "8. **Inference:**\n",
    "   - Implement an inference mechanism to use your trained model for translating new English sentences into Hindi. This involves feeding an English sentence into the trained model and obtaining the predicted Hindi translation.\n",
    "\n",
    "9. **Post-processing:**\n",
    "   - Convert the model's output back into human-readable text. This may involve detokenization, handling special characters, or fixing spacing issues.\n",
    "\n",
    "10. **Deployment:**\n",
    "    - Deploy your model for use in real-world scenarios, such as through a web interface, API, or integration into other applications.\n",
    "\n",
    "Popular pre-trained models for machine translation, such as MarianMT or mBART, can also be fine-tuned for English to Hindi translation if you have a smaller dataset.\n",
    "\n",
    "Keep in mind that building an effective translation model requires substantial computational resources, data, and expertise in deep learning and NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace ?\n",
    "\n",
    "Hugging Face is a company and an open-source community that is known for its contributions to the field of Natural Language Processing (NLP). They have developed a platform and a repository that provide access to pre-trained models, datasets, and various tools related to NLP. The platform facilitates the sharing and usage of state-of-the-art models, making them accessible to developers, researchers, and practitioners.\n",
    "\n",
    "Here are some key aspects of Hugging Face:\n",
    "\n",
    "1. **Transformers Library:**\n",
    "   - Hugging Face is particularly famous for its Transformers library, which is an open-source library that provides a collection of pre-trained models for various NLP tasks such as text classification, named entity recognition, translation, summarization, and more.\n",
    "\n",
    "2. **Model Hub:**\n",
    "   - The Hugging Face Model Hub is a repository where users can find, share, and use pre-trained models. It includes models developed by Hugging Face and the broader community. This makes it easy for developers to access and utilize powerful NLP models without having to train them from scratch.\n",
    "\n",
    "3. **Tokenizers:**\n",
    "   - Hugging Face provides efficient tokenization tools, allowing users to preprocess text data in a way that is compatible with their models. These tokenizers are designed to handle large-scale datasets efficiently.\n",
    "\n",
    "4. **Training Pipelines:**\n",
    "   - The platform also supports training custom models. Users can fine-tune pre-trained models on their specific datasets or train models from scratch using the tools and resources provided by Hugging Face.\n",
    "\n",
    "5. **Community and Collaboration:**\n",
    "   - Hugging Face has a vibrant and active community of developers, researchers, and NLP enthusiasts. The collaborative nature of the platform encourages sharing of models, ideas, and improvements.\n",
    "\n",
    "6. **APIs and Services:**\n",
    "   - Hugging Face offers APIs and services that allow users to integrate NLP models into their applications and workflows seamlessly. This includes hosted model inference services.\n",
    "\n",
    "Developers often use Hugging Face's resources to leverage powerful NLP models, saving time and resources compared to training models from scratch. The platform has played a significant role in democratizing access to advanced NLP capabilities and fostering collaboration in the NLP research and development community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Library ?\n",
    "\n",
    "The Hugging Face Transformers library is an open-source library that provides a collection of pre-trained models and tools for working with state-of-the-art natural language processing (NLP) models, with a particular focus on transformers. Transformers are a type of deep learning architecture that has proven highly effective in a variety of NLP tasks.\n",
    "\n",
    "Here are key components and features of the Hugging Face Transformers library:\n",
    "\n",
    "1. **Model Zoo:**\n",
    "   - The library includes a model zoo with a wide range of pre-trained transformer models, including BERT, GPT, RoBERTa, T5, and more. These models are trained on massive datasets and can be fine-tuned for specific downstream tasks or used for various NLP applications.\n",
    "\n",
    "2. **Tokenizers:**\n",
    "   - Hugging Face provides efficient tokenization tools to preprocess text data in a format compatible with transformer models. These tokenizers are designed to handle large-scale datasets efficiently and can be used in conjunction with the library's pre-trained models.\n",
    "\n",
    "3. **Training Pipelines:**\n",
    "   - The library supports training pipelines for fine-tuning pre-trained models on custom datasets. Users can fine-tune models for tasks such as text classification, named entity recognition, translation, summarization, and more.\n",
    "\n",
    "4. **Inference:**\n",
    "   - Hugging Face Transformers facilitates easy inference with pre-trained models. Users can use the library to generate predictions or embeddings for new text data using the models available in the model zoo.\n",
    "\n",
    "5. **Model Configurations:**\n",
    "   - The library allows users to access and modify the configurations of pre-trained models. This includes parameters such as the model architecture, hyperparameters, and other settings.\n",
    "\n",
    "6. **Integration with PyTorch and TensorFlow:**\n",
    "   - Hugging Face Transformers is compatible with both PyTorch and TensorFlow, making it flexible for users who prefer either deep learning framework. Users can seamlessly load and use pre-trained models with their preferred framework.\n",
    "\n",
    "7. **Community Contributions:**\n",
    "   - The library benefits from a large and active community of developers and researchers. Users can contribute to the library, share their models, and collaborate with others in the community.\n",
    "\n",
    "8. **Model Hub:**\n",
    "   - The Hugging Face Model Hub is an online repository where users can discover, share, and use models. It provides a centralized location for accessing pre-trained models and related resources.\n",
    "\n",
    "By providing a user-friendly interface, pre-trained models, and tools for working with transformers, the Hugging Face Transformers library has become a go-to resource for developers and researchers working on NLP tasks. It has significantly contributed to the accessibility and usability of advanced transformer models in the broader machine learning and NLP community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 11:07:06.669601: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 11:07:07.127156: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 11:07:07.129323: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 11:07:09.322291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, MarianMTModel, MarianTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the pre-trained MarianMT model and tokenizer for English to Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-en-hi'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a translation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_transformers(from_text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(from_text, return_tensors='pt', truncation=True)\n",
    "\n",
    "    # Generate translation\n",
    "    translation_ids = model.generate(**inputs)\n",
    "    translation_text = tokenizer.batch_decode(translation_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return translation_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "यह जाँच है!\n"
     ]
    }
   ],
   "source": [
    "translated_text = translate_transformers('This is Test!  ')\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradio?\n",
    "\n",
    "Gradio is an open-source Python library that simplifies the process of creating user interfaces for machine learning models. It is designed to be easy to use and allows developers to quickly build interactive and customizable UIs around their machine learning models, even if they have limited web development experience.\n",
    "\n",
    "Key features of Gradio include:\n",
    "\n",
    "1. **Simple Interface Building:**\n",
    "   - Gradio provides a high-level interface for creating UIs. Users can define input and output components for their models using a few lines of code.\n",
    "\n",
    "2. **Wide Range of Input Components:**\n",
    "   - Gradio supports various input components for different types of data, including text input, image upload, webcam input, sliders, checkboxes, and more. This makes it versatile for different types of machine learning models.\n",
    "\n",
    "3. **Real-time Updates:**\n",
    "   - Gradio allows for real-time updates of model predictions as users interact with the input components. This is useful for demonstrating the model's behavior dynamically.\n",
    "\n",
    "4. **Support for Multiple Frameworks:**\n",
    "   - Gradio is framework-agnostic and can be used with popular machine learning frameworks such as TensorFlow, PyTorch, Scikit-Learn, and others. This flexibility makes it suitable for a wide range of models.\n",
    "\n",
    "5. **Integration with Pre-trained Models:**\n",
    "   - Gradio easily integrates with pre-trained models, allowing users to build interfaces around models they have developed or models available in the Hugging Face Model Hub and other repositories.\n",
    "\n",
    "6. **Shareable Interfaces:**\n",
    "   - Gradio provides a platform for sharing interfaces, making it easy to showcase and share your machine learning models with others. Interfaces can be shared through a link, allowing users to interact with the models without needing to run any code.\n",
    "\n",
    "Here's a simple example of using Gradio to create a UI for an image classification model:\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "\n",
    "# Define a simple image classification model function\n",
    "def classify_image(image):\n",
    "    # Your model inference logic here\n",
    "    # Return the predicted class or label\n",
    "    return \"Prediction: Class X\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(fn=classify_image, inputs=\"image\", outputs=\"text\")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n",
    "```\n",
    "\n",
    "In this example, `classify_image` is a function that takes an image as input and returns a text output. The `gr.Interface` is then used to create a simple UI for this function.\n",
    "\n",
    "Gradio is a useful tool for quickly prototyping and sharing machine learning models, especially when a visual interface can enhance the understanding and usability of the model. It's commonly used in educational settings, demonstrations, and applications where user interaction is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Interface setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=translate_transformers,\n",
    "    inputs=gr.Textbox(lines=2, placeholder='Text to translate'),\n",
    "    outputs='text'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Launch the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
